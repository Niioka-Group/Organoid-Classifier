{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split\n",
    "import timm\n",
    "import cv2\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import model_selection, metrics\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations.core.transforms_interface import DualTransform\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from albumentations.augmentations import functional as F\n",
    "from PIL import Image, ImageOps, ImageEnhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.11.2+cu111', '1.10.1+cu111')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__, torch.__version__ # ('0.11.2+cu111', '1.10.0+cu111')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Seeds basic parameters for reproductibility of results\n",
    "    \n",
    "    Arguments:\n",
    "        seed {int} -- Number of the seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device on which we are:cuda\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "\n",
    "  epochs =20                              # No. of epochs for training the model\n",
    "  lr = 0.001                              # Learning rate\n",
    "  BATCH_SIZE = 50\n",
    "  N_EPOCHS = 30                        # Batch Size for Dataset\n",
    "\n",
    "#   model_name = 'tf_efficientnet_b4_ns'    # Model name (we are going to import model from timm)\n",
    "  IMG=224\n",
    "  img_size = (IMG,IMG)                        # Resize all the images to be 224 by 224\n",
    "  \n",
    "  # going to be used for loading dataset\n",
    "  ds_path = \"path/to/image\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device on which we are:{}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "Base_dir=[]\n",
    "import os\n",
    "        \n",
    "\n",
    "for category in ['A','B','C']:\n",
    "    for p in os.listdir(os.path.join(CFG.ds_path, category)):\n",
    "        f = Path(os.path.join(CFG.ds_path,category, p))\n",
    "        Base_dir.append(str(f))\n",
    "        labels.append(str(f.parent.stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir, test_dir = train_test_split(Base_dir, test_size=0.2,random_state=123,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import albumentations\n",
    "import albumentations.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformer=albumentations.Compose([\n",
    "    albumentations.Resize(224,224),\n",
    "    albumentations.pytorch.ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, filelist, transform = None):\n",
    "        self.filelist = filelist\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.filelist))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imgpath = self.filelist[index]\n",
    "        img = cv2.imread(imgpath)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if \"A\" in imgpath[-20:]:\n",
    "            label = 0\n",
    "        elif \"B\" in imgpath[-20:]:\n",
    "            label = 1\n",
    "        elif \"C\" in imgpath[-20:]:\n",
    "            label=2\n",
    "\n",
    "        if self.transform:\n",
    "            res = self.transform(image=img)\n",
    "            img = res['image']\n",
    "        else:\n",
    "            img = img\n",
    "\n",
    "        return (img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=Dataset(test_dir,test_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader=DataLoader(test,batch_size=CFG.BATCH_SIZE,num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtained from Paper ###\n",
    "# Configs taken from \n",
    "# https://github.com/leondgarse/keras_efficientnet_v2/blob/main/keras_efficientnet_v2/efficientnet_v2.py\n",
    "# convs parameter is which type of block to use, maps to `layer_map`\n",
    "# 6 models are supported, more are coming soon.\n",
    "CONFIGS = {\n",
    "    \"b0\": {\n",
    "        \"widths\": [32, 16, 32, 48, 96, 112, 192],\n",
    "        \"depths\": [1, 2, 2, 3, 5, 8],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"convs\": [0, 1, 1, 2, 3, 3],\n",
    "        \"output_conv_size\": 1280,\n",
    "        \"timm_weights\": \"tf_efficientnetv2_b0\",\n",
    "    },\n",
    "    \"b1\": {\n",
    "        \"widths\": [32, 16, 32, 48, 96, 112, 192],\n",
    "        \"depths\": [2, 3, 3, 4, 6, 9],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"convs\":[0, 1, 1, 2, 3, 3],\n",
    "        \"output_conv_size\": 1280,\n",
    "        \"timm_weights\":\"tf_efficientnetv2_b1\",\n",
    "    },\n",
    "    \"b2\": {\n",
    "        \"widths\": [32, 16, 32, 56, 104, 120, 208],\n",
    "        \"depths\": [2, 3, 3, 4, 6, 10],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"convs\": [0, 1, 1, 2, 3, 3],\n",
    "        \"output_conv_size\": 1408,\n",
    "        \"timm_weights\":\"tf_efficientnetv2_b2\",\n",
    "    },\n",
    "    \"s\": {\n",
    "        \"widths\": [24, 24, 48, 64, 128, 160, 256],\n",
    "        \"depths\": [2, 4, 4, 6, 9, 15],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"convs\": [0, 1, 1, 2, 3, 3],\n",
    "        \"output_conv_size\": 1280,\n",
    "        \"timm_weights\": \"tf_efficientnetv2_s\"\n",
    "    },\n",
    "    \"m\": {\n",
    "        \"widths\": [24, 24, 48, 80, 160, 176, 304, 512],\n",
    "        \"depths\": [3, 5, 5, 7, 14, 18, 5],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"convs\": [0, 1, 1, 2, 3, 3, 3],\n",
    "        \"output_conv_size\": 1280,\n",
    "        \"timm_weights\":\"tf_efficientnetv2_m\",\n",
    "    },\n",
    "    \"l\": {\n",
    "        \"widths\": [32, 32, 64, 96, 192, 224, 384, 640],\n",
    "        \"depths\": [4, 7, 7, 10, 19, 25, 7],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"convs\": [0, 1, 1, 2, 3, 3, 3],\n",
    "        \"output_conv_size\": 1280,\n",
    "        \"timm_weights\":\"tf_efficientnetv2_l\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, kernel_size=3, \n",
    "               stride=1, padding=1, groups=1,\n",
    "               bias=False, bn=True, act = True):\n",
    "    layers = [\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, \n",
    "                  padding=padding, groups=groups, bias=bias),\n",
    "        nn.BatchNorm2d(out_channels) if bn else nn.Identity(),\n",
    "        nn.SiLU() if act else nn.Identity()\n",
    "    ]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, c, r=24):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveMaxPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Conv2d(c, c // r, kernel_size=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(c // r, c, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        s = self.squeeze(x)\n",
    "        e = self.excitation(s)\n",
    "        return x * e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBConv(nn.Module):\n",
    "    def __init__(self, n_in, n_out, expansion, kernel_size=3, stride=1, r=24, dropout=0.1):\n",
    "        super(MBConv, self).__init__()\n",
    "        self.skip_connection = (n_in == n_out) and (stride == 1)\n",
    "        \n",
    "        padding = (kernel_size-1)//2\n",
    "        expanded = expansion*n_in\n",
    "        \n",
    "        self.expand_pw = nn.Identity() if expansion == 1 else conv_block(n_in, expanded, kernel_size=1, padding=0)\n",
    "        self.depthwise = conv_block(expanded, expanded, kernel_size=kernel_size, \n",
    "                                    stride=stride, padding=padding, groups=expanded)\n",
    "        self.se = SEBlock(expanded, r=4 * expansion)\n",
    "        self.reduce_pw = conv_block(expanded, n_out, kernel_size=1, padding=0, act=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.expand_pw(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.reduce_pw(x)\n",
    "        if self.skip_connection:\n",
    "            x = self.dropout(x)\n",
    "            x = x + residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedMBConv(nn.Module):\n",
    "    def __init__(self, n_in, n_out, expansion, kernel_size=3, stride=1, r=24, dropout=0.1):\n",
    "        super(FusedMBConv, self).__init__()\n",
    "        self.skip_connection = (n_in == n_out) and (stride == 1)\n",
    "        padding = (kernel_size-1)//2\n",
    "        expanded = expansion*n_in\n",
    "        \n",
    "        self.expand_pw = conv_block(n_in, expanded, kernel_size=3, stride=stride, padding=1)\n",
    "        self.reduce_pw = conv_block(expanded, n_out, kernel_size=1, padding=0, act=False)\n",
    "        \n",
    "        if expansion == 1:\n",
    "            self.reduce_pw = nn.Identity() # for consistency with timm implementation\n",
    "            self.expand_pw = conv_block(n_in, n_out, kernel_size=3, stride=stride, padding=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.expand_pw(x)\n",
    "        x = self.reduce_pw(x)\n",
    "        if self.skip_connection:\n",
    "            x = self.dropout(x)\n",
    "            x = x + residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbconv4(n_in, n_out, kernel_size=3, stride=1, r=24, dropout=0.1):\n",
    "    return MBConv(n_in, n_out, 4, kernel_size=kernel_size, stride=stride, r=r, dropout=dropout)\n",
    "def mbconv6(n_in, n_out, kernel_size=3, stride=1, r=24, dropout=0.1):\n",
    "    return MBConv(n_in, n_out, 6, kernel_size=kernel_size, stride=stride, r=r, dropout=dropout)\n",
    "def fused_mbconv1(n_in, n_out, kernel_size=3, stride=1, r=24, dropout=0.1):\n",
    "    return FusedMBConv(n_in, n_out, 1, kernel_size=kernel_size, stride=stride, r=r, dropout=dropout)\n",
    "def fused_mbconv4(n_in, n_out, kernel_size=3, stride=1, r=24, dropout=0.1):\n",
    "    return FusedMBConv(n_in, n_out, 4, kernel_size=kernel_size, stride=stride, r=r, dropout=dropout)\n",
    "layers_map = [fused_mbconv1, fused_mbconv4, mbconv4, mbconv6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage(n_in, n_out, num_layers, layer=mbconv6, \n",
    "                 kernel_size=3, stride=1, r=24, ps=0):\n",
    "    layers = [layer(n_in, n_out, kernel_size=kernel_size,\n",
    "                       stride=stride, r=r, dropout=ps)]\n",
    "    layers += [layer(n_out, n_out, kernel_size=kernel_size,\n",
    "                        r=r, dropout=ps) for _ in range(num_layers-1)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic EfficientNet V2 Class.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, n_classes=3):\n",
    "        super(EfficientNetV2, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.n_classes = n_classes\n",
    "        widths, depths, strides, convs = cfg['widths'],cfg['depths'],cfg['strides'],cfg['convs']\n",
    "        outconv_size = cfg['output_conv_size']\n",
    "        \n",
    "        stages = [conv_block(3, widths[0], stride=2, padding=1)]\n",
    "        for i in range(len(depths)):\n",
    "            stages.append(create_stage(widths[i], widths[i + 1], depths[i], layer=layers_map[convs[i]], \n",
    "                        stride=strides[i], r=4 if i ==0 else 24, ps=0))\n",
    "\n",
    "        self.features = nn.Sequential(*stages)\n",
    "        self.pre = conv_block(widths[-1], outconv_size, kernel_size=1, padding=0)\n",
    "        self.pool_flatten = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(outconv_size, n_classes)\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pre(x)\n",
    "        x = self.pool_flatten(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientnetv2_b0(n_classes=3):\n",
    "    return EfficientNetV2(CONFIGS['b0'], n_classes=n_classes)\n",
    "def efficientnetv2_b1(n_classes=3):\n",
    "    return EfficientNetV2(CONFIGS['b1'], n_classes=n_classes)\n",
    "def efficientnetv2_b2(n_classes=3):\n",
    "    return EfficientNetV2(CONFIGS['b2'], n_classes=n_classes)\n",
    "def efficientnetv2_s(n_classes=3):\n",
    "    return EfficientNetV2(CONFIGS['s'], n_classes=n_classes)\n",
    "def efficientnetv2_m(n_classes=3):\n",
    "    return EfficientNetV2(CONFIGS['m'], n_classes=n_classes)\n",
    "def efficientnetv2_l(n_classes=3):\n",
    "    return EfficientNetV2(CONFIGS['l'], n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['A','B','C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timm_weights(model):\n",
    "    url = model.cfg.get(\"timm_weights\")\n",
    "    timm_model = timm.create_model(url, pretrained=True, num_classes=len(classes))\n",
    "    params = nn.utils.parameters_to_vector(timm_model.parameters())\n",
    "    nn.utils.vector_to_parameters(params, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efficientnetv2_s(n_classes=len(classes))\n",
    "load_timm_weights(model)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "n_inputs = model.head[0].in_features\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(n_inputs,512),\n",
    "    nn.SiLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, len(classes))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval(model):\n",
    "  test_loss = 0.0\n",
    "  correct_num=0.0\n",
    "  class_correct = list(0. for i in range(len(classes)))\n",
    "  class_total = list(0. for i in range(len(classes)))\n",
    "\n",
    "  model.eval()\n",
    "  test_loader=DataLoader(test,batch_size=CFG.BATCH_SIZE,num_workers=25)\n",
    "\n",
    "  for data, target in tqdm(test_loader):\n",
    "      if torch.cuda.is_available(): \n",
    "          data, target = data.cuda().float(), target.cuda()\n",
    "      with torch.no_grad():\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "      test_loss += loss.item()*data.size(0)\n",
    "      _, pred = torch.max(output, 1)    \n",
    "      correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "      correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "      correct_num+=torch.sum(correct_tensor)\n",
    "      for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "  test_acc=correct_num/len(test_loader.dataset)\n",
    "  print('Test acc: {:.6f}\\n'.format(test_acc))\n",
    "  test_loss = test_loss/len(test_loader.dataset)\n",
    "  print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "  for i in range(len(classes)):\n",
    "      if class_total[i] > 0:\n",
    "          print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "              classes[i], 100 * class_correct[i] / class_total[i],\n",
    "              np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "      else:\n",
    "          print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "  print('\\nTest Accuracy (Overall): {:.4f} ({}/{})'.format(\n",
    "      100. * np.sum(class_correct) / np.sum(class_total),\n",
    "      np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "test_loader=DataLoader(test,batch_size=CFG.BATCH_SIZE,num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_models=[]\n",
    "for i in range(5):\n",
    "    model.load_state_dict(torch.load(f'./weights/model_5e_{i}.pth'))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    eff_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Grad_Cam:\n",
    "    def __init__(self,model,list_of_dir=None) -> None:\n",
    "        self.features = model.features.eval()\n",
    "        self.pre= model.pre.eval()\n",
    "        self.avgpool = model.pool_flatten.eval()\n",
    "        self.classifier = model.head.eval()\n",
    "        self.list_dir=list_of_dir\n",
    "\n",
    "    def effv2_grad_cam(self,image_dir):\n",
    "        image = cv2.imread(image_dir)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        input_image = test_transformer(image = image)[\"image\"].float().unsqueeze(0).to(device)\n",
    "        # 特徴マップを抽出する\n",
    "        feature = self.features(input_image)\n",
    "        # 特徴マップサイズ torch.Size([1, 512, 7, 7])\n",
    "        # 上のteratailの回答を参照\n",
    "        feature = feature.clone().detach().requires_grad_(True)\n",
    "        # print('特徴マップサイズ', feature.size())\n",
    "        # 特徴マップをVGG19の残りの全結合層に通して、予測結果を得る\n",
    "        pres=self.pre(feature)\n",
    "        pooled = self.avgpool(pres)\n",
    "        y_pred = self.classifier(pooled.view(-1,1280))\n",
    "        pred_index = torch.argmax(y_pred)\n",
    "        # 予測結果に対して誤差逆伝播\n",
    "        y_pred[0][pred_index].backward()\n",
    "    \n",
    "        # 特徴マップの勾配(feature.grad)のGlobal Average Poolingを計算する\n",
    "        # つまり512枚の各特徴マップの要素の平均値を算出する（512次元のベクトルになる）\n",
    "        # まずは7x7のそれぞれの特徴マップを１本のベクトルに変換\n",
    "        feature_vec = feature.grad.view(256, 7*7) # feature_vec.size() = (512, 49)\n",
    "        # 512本のそれぞれのベクトルの要素の平均を取る\n",
    "        # 論文のαが計算される\n",
    "        alpha = torch.mean(feature_vec, axis=1) # alpha.size() = (512)\n",
    "        # print('alpha_size',alpha.shape,alpha)\n",
    "        # batch_sizeの次元を削除\n",
    "        # (1x256x7x7) -> (256x7x7)\n",
    "        feature = feature.squeeze(0)\n",
    "        # 論文のLを計算\n",
    "        h=torch.max(torch.sum(feature*alpha.view(-1,1,1),0))\n",
    "        if h>0:\n",
    "            L = F.relu(torch.sum(feature*alpha.view(-1,1,1),0)).cpu()\n",
    "        elif h<0:\n",
    "            L = F.relu(torch.sum(feature*alpha.view(-1,1,1),0)+h*(-2)).cpu()\n",
    "        else:\n",
    "            L = torch.sum(feature*alpha.view(-1,1,1),0).cpu()\n",
    "        L = L.detach().numpy()\n",
    "        # 0-1で正規化\n",
    "        L_min = np.min(L)\n",
    "        L_max = np.max(L - L_min)+0.00001\n",
    "        L = (L - L_min)/L_max\n",
    "\n",
    "        # 元画像と同じサイズにリサイズする\n",
    "        L = cv2.resize(L, (224, 224))\n",
    "        # heat map に変換\n",
    "        \n",
    "        img2 = self.toHeatmap(L)\n",
    "        img1 = input_image.squeeze(0).permute(1,2,0).cpu()/255\n",
    "        alpha = 0.5\n",
    "        grad_cam_image = (img1*alpha + img2*(1-alpha))\n",
    "        cat_tensor = torch.cat([grad_cam_image, img1], dim = 1)\n",
    "        # print(grad_cam_image.shape)\n",
    "        # plt.figure(figsize=(15, 15), dpi=50)\n",
    "        # plt.imshow(cat_tensor)\n",
    "        tmp = cat_tensor.to('cpu').detach().numpy().copy() \n",
    "        img_pil = Image.fromarray((tmp*255).astype(np.uint8))\n",
    "        dir_path='./grad_cam_images2/'+image_dir[37]\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "        #画像ファイルを出力して確認します\n",
    "        img_pil.save(dir_path+image_dir[38:])\n",
    "    def toHeatmap(self,x):\n",
    "        x = (x*255).reshape(-1)\n",
    "        cm = plt.get_cmap('jet')\n",
    "        x = np.array([cm(int(np.round(xi)))[:3] for xi in x])\n",
    "        return x.reshape(224,224,3)\n",
    "    def main(self):\n",
    "        for i in range(len(self.list_dir)):\n",
    "            self.effv2_grad_cam(self.list_dir[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam=Grad_Cam(eff_models[0],test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/meidai/model_code/eff/Grad_cam.ipynb Cell 37\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bziro/root/meidai/model_code/eff/Grad_cam.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m grad_cam\u001b[39m.\u001b[39;49mmain()\n",
      "\u001b[1;32m/root/meidai/model_code/eff/Grad_cam.ipynb Cell 37\u001b[0m line \u001b[0;36mGrad_Cam.main\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bziro/root/meidai/model_code/eff/Grad_cam.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bziro/root/meidai/model_code/eff/Grad_cam.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlist_dir)):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bziro/root/meidai/model_code/eff/Grad_cam.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffv2_grad_cam(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlist_dir[i])\n",
      "\u001b[1;32m/root/meidai/model_code/eff/Grad_cam.ipynb Cell 37\u001b[0m line \u001b[0;36mGrad_Cam.effv2_grad_cam\u001b[0;34m(self, image_dir)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bziro/root/meidai/model_code/eff/Grad_cam.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m h\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mmax(torch\u001b[39m.\u001b[39msum(feature\u001b[39m*\u001b[39malpha\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m),\u001b[39m0\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bziro/root/meidai/model_code/eff/Grad_cam.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mif\u001b[39;00m h\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bziro/root/meidai/model_code/eff/Grad_cam.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     L \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mrelu(torch\u001b[39m.\u001b[39;49msum(feature\u001b[39m*\u001b[39;49malpha\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m),\u001b[39m0\u001b[39;49m))\u001b[39m.\u001b[39;49mcpu()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bziro/root/meidai/model_code/eff/Grad_cam.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39melif\u001b[39;00m h\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bziro/root/meidai/model_code/eff/Grad_cam.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     L \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(torch\u001b[39m.\u001b[39msum(feature\u001b[39m*\u001b[39malpha\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m),\u001b[39m0\u001b[39m)\u001b[39m+\u001b[39mh\u001b[39m*\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grad_cam.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
