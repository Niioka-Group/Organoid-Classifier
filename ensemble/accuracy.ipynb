{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "plt.style.use(\"ggplot\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from vit_pytorch.efficient import ViT\n",
    "import seaborn as sns   #←これを追加\n",
    "import timm    #←これを追加\n",
    "from albumentations.core.transforms_interface import DualTransform\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from albumentations.augmentations import functional as F\n",
    "from PIL import Image, ImageOps, ImageEnhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "\n",
    "                       # No. of epochs for training the model\n",
    "  # Training settings\n",
    "  batch_size = 25\n",
    "  epochs = 20\n",
    "  lr = 3e-5\n",
    "  gamma = 0.7\n",
    "  seed = 42                      # Batch Size for Dataset\n",
    "\n",
    "#   model_name = 'tf_efficientnet_b4_ns'    # Model name (we are going to import model from timm)\n",
    "  IMG=224\n",
    "  img_size = (IMG,IMG)                        # Resize all the images to be 224 by 224\n",
    "  \n",
    "  # going to be used for loading dataset\n",
    "  ds_path = \"/path/to/image\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device on which we are:{}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(CFG.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "labels = []\n",
    "Base_dir=[]\n",
    "import os\n",
    "        \n",
    "\n",
    "for category in ['A','B','C']:\n",
    "    for p in os.listdir(os.path.join(CFG.ds_path, category)):\n",
    "        f = Path(os.path.join(CFG.ds_path,category, p))\n",
    "        Base_dir.append(str(f))\n",
    "        labels.append(str(f.parent.stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir, test_dir = train_test_split(Base_dir, test_size=0.2,random_state=123,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformer= transforms.Compose([\n",
    "    transforms.Resize(CFG.img_size),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['A','B','C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViT Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "model_names = timm.list_models(pretrained=True)\n",
    "pprint(model_names)\n",
    "vit_model = timm.create_model('vit_small_patch16_224', pretrained=True, num_classes=3)\n",
    "vit_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_models=[]\n",
    "for i in range(5):\n",
    "    vit_model.load_state_dict(torch.load(f'..vit/weights/model_5e_{i}.pth'))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "    vit_model = vit_model.to(device)\n",
    "    vit_model.eval()\n",
    "    vit_models.append(vit_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient net Configration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "### Obtained from Paper ###\n",
    "# Configs taken from \n",
    "# https://github.com/leondgarse/keras_efficientnet_v2/blob/main/keras_efficientnet_v2/efficientnet_v2.py\n",
    "# convs parameter is which type of block to use, maps to `layer_map`\n",
    "# 6 models are supported, more are coming soon.\n",
    "CONFIGS = {\n",
    "    \"b0\": {\n",
    "        \"widths\": [32, 16, 32, 48, 96, 112, 192],\n",
    "        \"depths\": [1, 2, 2, 3, 5, 8],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"convs\": [0, 1, 1, 2, 3, 3],\n",
    "        \"output_conv_size\": 1280,\n",
    "        \"timm_weights\": \"tf_efficientnetv2_b0\",\n",
    "    },\n",
    "    \"b1\": {\n",
    "        \"widths\": [32, 16, 32, 48, 96, 112, 192],\n",
    "        \"depths\": [2, 3, 3, 4, 6, 9],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"convs\":[0, 1, 1, 2, 3, 3],\n",
    "        \"output_conv_size\": 1280,\n",
    "        \"timm_weights\":\"tf_efficientnetv2_b1\",\n",
    "    },\n",
    "    \"b2\": {\n",
    "        \"widths\": [32, 16, 32, 56, 104, 120, 208],\n",
    "        \"depths\": [2, 3, 3, 4, 6, 10],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"convs\": [0, 1, 1, 2, 3, 3],\n",
    "        \"output_conv_size\": 1408,\n",
    "        \"timm_weights\":\"tf_efficientnetv2_b2\",\n",
    "    },\n",
    "    \"s\": {\n",
    "        \"widths\": [24, 24, 48, 64, 128, 160, 256],\n",
    "        \"depths\": [2, 4, 4, 6, 9, 15],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"convs\": [0, 1, 1, 2, 3, 3],\n",
    "        \"output_conv_size\": 1280,\n",
    "        \"timm_weights\": \"tf_efficientnetv2_s\"\n",
    "    },\n",
    "    \"m\": {\n",
    "        \"widths\": [24, 24, 48, 80, 160, 176, 304, 512],\n",
    "        \"depths\": [3, 5, 5, 7, 14, 18, 5],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"convs\": [0, 1, 1, 2, 3, 3, 3],\n",
    "        \"output_conv_size\": 1280,\n",
    "        \"timm_weights\":\"tf_efficientnetv2_m\",\n",
    "    },\n",
    "    \"l\": {\n",
    "        \"widths\": [32, 32, 64, 96, 192, 224, 384, 640],\n",
    "        \"depths\": [4, 7, 7, 10, 19, 25, 7],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"convs\": [0, 1, 1, 2, 3, 3, 3],\n",
    "        \"output_conv_size\": 1280,\n",
    "        \"timm_weights\":\"tf_efficientnetv2_l\",\n",
    "    }\n",
    "}\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, n_in, n_out, expansion, kernel_size=3, stride=1, r=24, dropout=0.1):\n",
    "        super(MBConv, self).__init__()\n",
    "        self.skip_connection = (n_in == n_out) and (stride == 1)\n",
    "        \n",
    "        padding = (kernel_size-1)//2\n",
    "        expanded = expansion*n_in\n",
    "        \n",
    "        self.expand_pw = nn.Identity() if expansion == 1 else conv_block(n_in, expanded, kernel_size=1, padding=0)\n",
    "        self.depthwise = conv_block(expanded, expanded, kernel_size=kernel_size, \n",
    "                                    stride=stride, padding=padding, groups=expanded)\n",
    "        self.se = SEBlock(expanded, r=4 * expansion)\n",
    "        self.reduce_pw = conv_block(expanded, n_out, kernel_size=1, padding=0, act=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.expand_pw(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.reduce_pw(x)\n",
    "        if self.skip_connection:\n",
    "            x = self.dropout(x)\n",
    "            x = x + residual\n",
    "        return x\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, \n",
    "               stride=1, padding=1, groups=1,\n",
    "               bias=False, bn=True, act = True):\n",
    "    layers = [\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, \n",
    "                  padding=padding, groups=groups, bias=bias),\n",
    "        nn.BatchNorm2d(out_channels) if bn else nn.Identity(),\n",
    "        nn.SiLU() if act else nn.Identity()\n",
    "    ]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, c, r=24):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveMaxPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Conv2d(c, c // r, kernel_size=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(c // r, c, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        s = self.squeeze(x)\n",
    "        e = self.excitation(s)\n",
    "        return x * e\n",
    "\n",
    "class FusedMBConv(nn.Module):\n",
    "    def __init__(self, n_in, n_out, expansion, kernel_size=3, stride=1, r=24, dropout=0.1):\n",
    "        super(FusedMBConv, self).__init__()\n",
    "        self.skip_connection = (n_in == n_out) and (stride == 1)\n",
    "        padding = (kernel_size-1)//2\n",
    "        expanded = expansion*n_in\n",
    "        \n",
    "        self.expand_pw = conv_block(n_in, expanded, kernel_size=3, stride=stride, padding=1)\n",
    "        self.reduce_pw = conv_block(expanded, n_out, kernel_size=1, padding=0, act=False)\n",
    "        \n",
    "        if expansion == 1:\n",
    "            self.reduce_pw = nn.Identity() # for consistency with timm implementation\n",
    "            self.expand_pw = conv_block(n_in, n_out, kernel_size=3, stride=stride, padding=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.expand_pw(x)\n",
    "        x = self.reduce_pw(x)\n",
    "        if self.skip_connection:\n",
    "            x = self.dropout(x)\n",
    "            x = x + residual\n",
    "        return x\n",
    "\n",
    "def mbconv4(n_in, n_out, kernel_size=3, stride=1, r=24, dropout=0.1):\n",
    "    return MBConv(n_in, n_out, 4, kernel_size=kernel_size, stride=stride, r=r, dropout=dropout)\n",
    "def mbconv6(n_in, n_out, kernel_size=3, stride=1, r=24, dropout=0.1):\n",
    "    return MBConv(n_in, n_out, 6, kernel_size=kernel_size, stride=stride, r=r, dropout=dropout)\n",
    "def fused_mbconv1(n_in, n_out, kernel_size=3, stride=1, r=24, dropout=0.1):\n",
    "    return FusedMBConv(n_in, n_out, 1, kernel_size=kernel_size, stride=stride, r=r, dropout=dropout)\n",
    "def fused_mbconv4(n_in, n_out, kernel_size=3, stride=1, r=24, dropout=0.1):\n",
    "    return FusedMBConv(n_in, n_out, 4, kernel_size=kernel_size, stride=stride, r=r, dropout=dropout)\n",
    "layers_map = [fused_mbconv1, fused_mbconv4, mbconv4, mbconv6]\n",
    "\n",
    "\n",
    "def create_stage(n_in, n_out, num_layers, layer=mbconv6, \n",
    "                 kernel_size=3, stride=1, r=24, ps=0):\n",
    "    layers = [layer(n_in, n_out, kernel_size=kernel_size,\n",
    "                       stride=stride, r=r, dropout=ps)]\n",
    "    layers += [layer(n_out, n_out, kernel_size=kernel_size,\n",
    "                        r=r, dropout=ps) for _ in range(num_layers-1)]\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic EfficientNet V2 Class.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, n_classes=3):\n",
    "        super(EfficientNetV2, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.n_classes = n_classes\n",
    "        widths, depths, strides, convs = cfg['widths'],cfg['depths'],cfg['strides'],cfg['convs']\n",
    "        outconv_size = cfg['output_conv_size']\n",
    "        \n",
    "        stages = [conv_block(3, widths[0], stride=2, padding=1)]\n",
    "        for i in range(len(depths)):\n",
    "            stages.append(create_stage(widths[i], widths[i + 1], depths[i], layer=layers_map[convs[i]], \n",
    "                        stride=strides[i], r=4 if i ==0 else 24, ps=0))\n",
    "\n",
    "        self.features = nn.Sequential(*stages)\n",
    "        self.pre = conv_block(widths[-1], outconv_size, kernel_size=1, padding=0)\n",
    "        self.pool_flatten = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(outconv_size, n_classes)\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pre(x)\n",
    "        x = self.pool_flatten(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientnetv2_b0(n_classes=3):\n",
    "    return EfficientNetV2(CONFIGS['b0'], n_classes=n_classes)\n",
    "def efficientnetv2_b1(n_classes=3):\n",
    "    return EfficientNetV2(CONFIGS['b1'], n_classes=n_classes)\n",
    "def efficientnetv2_b2(n_classes=3):\n",
    "    return EfficientNetV2(CONFIGS['b2'], n_classes=n_classes)\n",
    "def efficientnetv2_s(n_classes=3):\n",
    "    return EfficientNetV2(CONFIGS['s'], n_classes=n_classes)\n",
    "def efficientnetv2_m(n_classes=3):\n",
    "    return EfficientNetV2(CONFIGS['m'], n_classes=n_classes)\n",
    "def efficientnetv2_l(n_classes=3):\n",
    "    return EfficientNetV2(CONFIGS['l'], n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timm_weights(model):\n",
    "    url = model.cfg.get(\"timm_weights\")\n",
    "    timm_model = timm.create_model(url, pretrained=True, num_classes=len(classes))\n",
    "    params = nn.utils.parameters_to_vector(timm_model.parameters())\n",
    "    nn.utils.vector_to_parameters(params, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_model = efficientnetv2_s(n_classes=len(classes))\n",
    "load_timm_weights(eff_model)\n",
    "for param in eff_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "n_inputs = eff_model.head[0].in_features\n",
    "eff_model.head = nn.Sequential(\n",
    "    nn.Linear(n_inputs,512),\n",
    "    nn.SiLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, len(classes))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_models=[]\n",
    "for i in range(5):\n",
    "    eff_model.load_state_dict(torch.load(f'./weights/model_5e_{i}.pth'))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "    eff_model = eff_model.to(device)\n",
    "    eff_model.eval()\n",
    "    eff_models.append(eff_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, filelist, transform = None):\n",
    "        self.filelist = filelist\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.filelist))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imgpath = self.filelist[index]\n",
    "        img = Image.open(imgpath).convert(mode=\"RGB\")\n",
    "\n",
    "        if \"A\" in imgpath[-20:]:\n",
    "            label = 0\n",
    "        elif \"B\" in imgpath[-20:]:\n",
    "            label = 1\n",
    "        elif \"C\" in imgpath[-20:]:\n",
    "            label=2\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=Dataset(test_dir,test_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "def test_eval(model):\n",
    "  test_loss = 0.0\n",
    "  correct_num=0.0\n",
    "  class_correct = list(0. for i in range(len(classes)))\n",
    "  class_total = list(0. for i in range(len(classes)))\n",
    "\n",
    "  model.eval()\n",
    "  test_loader=DataLoader(test,batch_size=CFG.batch_size,num_workers=25)\n",
    "\n",
    "  for data, target in tqdm(test_loader):\n",
    "      if torch.cuda.is_available(): \n",
    "          data, target = data.cuda(), target.cuda()\n",
    "      with torch.no_grad():\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "      test_loss += loss.item()*data.size(0)\n",
    "      _, pred = torch.max(output, 1)    \n",
    "      correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "      correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "      correct_num+=torch.sum(correct_tensor)\n",
    "      for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "  test_acc=correct_num/len(test_loader.dataset)\n",
    "  # print('Test acc: {:.6f}\\n'.format(test_acc))\n",
    "  test_loss = test_loss/len(test_loader.dataset)\n",
    "  print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "  for i in range(len(classes)):\n",
    "      if class_total[i] > 0:\n",
    "          print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "              classes[i], 100 * class_correct[i] / class_total[i],\n",
    "              np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "      else:\n",
    "          print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "  print('\\nTest Accuracy (Overall): {:.4f} ({}/{})'.format(\n",
    "      100. * np.sum(class_correct) / np.sum(class_total),\n",
    "      np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "def get_all_preds_k_fold(models,loader):\n",
    "  all_preds=torch.tensor([])\n",
    "  ground_truth=torch.tensor([])\n",
    "  running_loss=0\n",
    "  model1=models[0]\n",
    "  model2=models[1]\n",
    "  model3=models[2]\n",
    "  model4=models[3]\n",
    "  model5=models[4]\n",
    "  for batch in loader:\n",
    "    images,labels=batch\n",
    "    images=images.to(device)\n",
    "    labels=labels.to('cpu')\n",
    "\n",
    "    ground_truth=torch.cat((ground_truth,labels),dim=0)\n",
    "    p1=model1(images).to('cpu')\n",
    "    p2=model2(images).to('cpu')\n",
    "    p3=model3(images).to('cpu')\n",
    "    p4=model4(images).to('cpu')\n",
    "    p5=model5(images).to('cpu')\n",
    "    preds=(p1+p2+p3+p4+p5)/5\n",
    "\n",
    "    all_preds=torch.cat((all_preds,preds),dim=0)\n",
    "    loss_val = criterion(preds, labels)\n",
    "    running_loss += loss_val.item() * images.size(0)\n",
    "  \n",
    "  loss=running_loss/len(loader)\n",
    "  return all_preds,ground_truth,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "with torch.no_grad():\n",
    "      prediction_loader=torch.utils.data.DataLoader(test,batch_size=len(test),num_workers=16,shuffle=False)\n",
    "      vit_preds,ground_truth,loss=get_all_preds_k_fold(vit_models,prediction_loader)\n",
    "      eff_preds,ground_truth,loss=get_all_preds_k_fold(eff_models,prediction_loader)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_preds=(vit_preds+eff_preds)/2\n",
    "def get_num_correct(preds,labels):\n",
    "  print(preds.argmax(dim=1).eq(labels).sum().item()/len(preds))\n",
    "\n",
    "get_num_correct(ensemble_preds,ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(ground_truth,ensemble_preds.argmax(dim=1))\n",
    "print(type(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),fontsize=30, horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=('A','B','C')\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
